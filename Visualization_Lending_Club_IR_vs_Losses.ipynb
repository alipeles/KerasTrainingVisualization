{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./images/TrainedModel.png \"Image produced by callback\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Training Visualization\n",
    "\n",
    "This notebook uses a Keras callback and Matplotlib to display an animated graph of a model being trained.<br />\n",
    "\n",
    "The model trained is a linear regression (modeled as a single node neural network). The callback function in the cell titled \"Function that draws and updates the graph\" is generic and can be used for any neural network. The key limitations are that it's only for a single input and single output.<br />\n",
    "\n",
    "The basic strategy is to create a hook after each mini-batch training. The callback runs the model on the dataset and plots the results and (if desired) the current mean squared error.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Requirements\n",
    "Note that this uses qt5 for display, not inline. It is possible to do animation inline, but it's a bit more limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib qt5\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "This is publicly available data from the Lending Club (http://bit.ly/2LC6wth) on the performance of loans that they issued from 2017 to present. Interest rate was provided. I computed the total losses from fields in their file, did a bit of cleanup, and pickled it in this file. My data set is restricted to loans that have defaulted (aka \"charged off\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_rate</th>\n",
       "      <th>loss_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1527</td>\n",
       "      <td>0.768256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.937043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.823038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.810327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1065</td>\n",
       "      <td>0.392143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    int_rate  loss_pct\n",
       "1     0.1527  0.768256\n",
       "8     0.2128  0.937043\n",
       "9     0.1269  0.823038\n",
       "12    0.1349  0.810327\n",
       "14    0.1065  0.392143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data/lend_club_ir_v_losses.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = data['int_rate'].values\n",
    "labels = data['loss_pct'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.reshape(-1, 1), labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that determines how often to update the graph\n",
    "Designed spefically for the one-parameter linear model below, this function asks for an update whenever the weight or bias has changed by more than a specified threshold (defaults to 1%). A minimum frequency (defaults to once every 500 batches) can also be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# returns closured callback that will determine how often the graph is redrawn\n",
    "def get_frequency_callback(**kwargs):\n",
    "\n",
    "    # parameters\n",
    "    weight_threshold = kwargs.get('weight_threshold', 0.01)\n",
    "    bias_threshold = kwargs.get('bias_threshold', 0.01)\n",
    "    min_frequency = kwargs.get('min_frequency', 500)\n",
    "\n",
    "    # declared variables that will be retained between invocations of the callback\n",
    "    layer = None\n",
    "    w_prev = 0\n",
    "    b_prev = 0\n",
    "    batch_prev = 0\n",
    "    \n",
    "    # the callback that will actually make the decision to update or not\n",
    "    def frequency_callback(model, X, y, tot_batches):\n",
    "\n",
    "        nonlocal layer, w_prev, b_prev, batch_prev\n",
    "        \n",
    "        # get the model layer containing the weight and bias\n",
    "        if layer == None:\n",
    "            layer = model.get_layer('output')\n",
    "        \n",
    "        # get the current value of the weight and bias\n",
    "        w = layer.get_weights()[0][0][0]\n",
    "        b = layer.get_weights()[1][0]\n",
    "\n",
    "        # assume change was too small for an update\n",
    "        display = False\n",
    "        \n",
    "        # if change in weight or bias exceeds relevant threshold, or it's been too long since the last update \n",
    "        if (np.abs(w - w_prev) > weight_threshold or np.abs(b - b_prev) > bias_threshold) \\\n",
    "            or tot_batches - batch_prev > min_frequency:\n",
    "            \n",
    "            # update on this iteration\n",
    "            display = True\n",
    "            \n",
    "            # keep track of weight and bias from last update\n",
    "            w_prev = w\n",
    "            b_prev = b\n",
    "            \n",
    "            # keep track of how long since the last update\n",
    "            batch_prev = tot_batches\n",
    "        \n",
    "        return display\n",
    "    \n",
    "    # return closure\n",
    "    return frequency_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that draws and updates the graph\n",
    "The callback function should be run *on_batch_end*. It determines whether an update is necessary and redraws the graph, displaying related data, as well.<br />\n",
    "\n",
    "The enclosing function does some set up and creates a closure with the data that will be retained between calls or that needs to be known in advance, because keras passes very little information to the callback.<br />\n",
    "\n",
    "This function should be reusable for graphing any single-input single-output model in Keras. I have used it, e.g., for multi-layer neural networks.<br />\n",
    "\n",
    "Expect this to be *super slow*. Especially for simple models, the cost of running and graphing the model will be significantly higher than the cost of training on a single batch. You will get warnings from Keras about slowness.<br />\n",
    "\n",
    "This is a toy for learning / investigating. So, performance is not a primary concern, but you do want it to be usable. Several things can have a big impact on performance.<br />\n",
    "<ol>\n",
    "<li>The **sparsity** options reduce the number datapoints used in the scatter plot and in running the model on each pass.</li>\n",
    "<li>The **frequency** option determines how often to update the graph. I've used the function option to update only when the change to the model parameters is big enough to justify an update. But, it's specific to the model being trained.</li>\n",
    "<li>Turning off the **error display** slightly reduces the number of computations, but significantly reduces the amount of drawing.</li>\n",
    "</ol>\n",
    "\n",
    "There is also an option to write the updates to files as individual images, which can then be used to create an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# prevents overflow when displaying a lot of data points\n",
    "mpl.rcParams['agg.path.chunksize'] = 100000\n",
    "\n",
    "# do some set up and return a closure around the redraw callback for Keras \n",
    "def get_redraw(X_in, y_in, model, batch_size, epochs, **kwargs):\n",
    "\n",
    "    ## PROCESS COMMAND LINE ARGUMENTS\n",
    "    \n",
    "    # plot dimensions\n",
    "    left = kwargs.get('left', X_in.min())\n",
    "    right = kwargs.get('right', X_in.max())\n",
    "    bottom = kwargs.get('bottom', y_in.min())\n",
    "    top = kwargs.get('top', y_in.max())\n",
    "\n",
    "    # how much data to use in graph\n",
    "    \n",
    "    # ... scatter plot sparsity (0 = no scatter plot)\n",
    "    #     scatter is only drawn once, but it can be a lot of data, both computationally and visually\n",
    "    scatter_sparsity = kwargs.get('scatter_sparsity', 5)\n",
    "\n",
    "    # ... graph sparsity\n",
    "    #     keeping the graph sparse improves performance\n",
    "    graph_sparsity = kwargs.get('graph_sparsity', 1000)\n",
    "\n",
    "    # whether to display error\n",
    "    show_err = kwargs.get('show_err', True)\n",
    "    \n",
    "    # .. and level of smoothing to apply to error, if so (needs to be an odd number)\n",
    "    err_smoothing = kwargs.get('err_smoothing', 101)\n",
    "    \n",
    "    # how frequently (in batches) to update the graph\n",
    "    frequency = kwargs.get('frequency', 10)\n",
    "    if callable(frequency):\n",
    "        # if a function is provided, it will be called every batch and asked for a True/False response\n",
    "        frequency_mode = 'function'\n",
    "    elif np.isscalar(frequency):\n",
    "        # if a number is provided, updates will be done every [frequency] batches\n",
    "        frequency_mode = 'scalar'\n",
    "    else:\n",
    "        # for array-like setting, update when frequency[batch number] is True\n",
    "        frequency_mode = 'array'\n",
    "        \n",
    "    # figure size\n",
    "    figure_size = kwargs.get('figure_size', (15, 10))\n",
    "    \n",
    "    # text labels\n",
    "    title = kwargs.get('title', None)\n",
    "    x_label = kwargs.get('x_label', None)\n",
    "    y_label = kwargs.get('y_label', None)\n",
    "    \n",
    "    # tick formatters\n",
    "    x_tick_formatter = kwargs.get('x_tick_formatter', None)\n",
    "    y_tick_formatter = kwargs.get('y_tick_formatter', None)\n",
    "    \n",
    "    # loss scale (depends on loss function)\n",
    "    loss_scale = kwargs.get('loss_scale', 1.0)\n",
    "    \n",
    "    # display legend\n",
    "    show_legend = kwargs.get('show_legend', True)\n",
    "    \n",
    "    # write to screen or file?\n",
    "    display_mode = kwargs.get('display_mode', 'screen')\n",
    "    filepath = kwargs.get('filepath', 'images/batch')\n",
    "    \n",
    "    \n",
    "    ## PREP DATA FOR QUICKER DISPLAY\n",
    "    \n",
    "    # parallel sort feature and label arrays for plotting\n",
    "    ix = X_in.argsort(axis=0)[:,0]\n",
    "\n",
    "    # ... reducing number of points used in graph\n",
    "    ix = ix[::graph_sparsity]\n",
    "    \n",
    "    X = X_in[ix]\n",
    "    y = y_in[ix]\n",
    "    \n",
    "    # keep track of total number of batches seens\n",
    "    tot_batches = 0\n",
    "    batches_per_epoch = np.ceil(len(X_in) / batch_size)\n",
    "    \n",
    "    # scale for loss plot = total number of batches that will be run\n",
    "    max_batches = epochs * batches_per_epoch\n",
    "    \n",
    "    ## DRAW BACKGROUND COMPONENTS\n",
    "    \n",
    "    # set the figure size and layout\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    grd = gridspec.GridSpec(ncols=3, nrows=2)\n",
    "    \n",
    "    # graphs for data/error, loss over time, and model parameters\n",
    "    ax_main = fig.add_subplot(grd[:2, :2])\n",
    "    ax_loss = fig.add_subplot(grd[:1, 2:])\n",
    "    ax_params = fig.add_subplot(grd[1:, 2:])\n",
    "\n",
    "    # data boundaries on main graph\n",
    "    ax_main.set_xlim(left, right)\n",
    "    ax_main.set_ylim(bottom, top)\n",
    "    \n",
    "    # titles and labels on main graph\n",
    "    if title:\n",
    "        ax_main.set_title(title, size=14, fontweight='bold', y=1.03)\n",
    "    \n",
    "    if x_label:\n",
    "        ax_main.set_xlabel(x_label, size=12, fontweight='bold')\n",
    "        \n",
    "    if y_label:\n",
    "        ax_main.set_ylabel(y_label, size=12, fontweight='bold')\n",
    "                 \n",
    "    # tick formatting on main graph\n",
    "    if x_tick_formatter:\n",
    "        ax_main.xaxis.set_major_formatter(x_tick_formatter)\n",
    "        \n",
    "    if y_tick_formatter:\n",
    "        ax_main.yaxis.set_major_formatter(y_tick_formatter)\n",
    "        \n",
    "    # draw a scatter plot of the training data on main graph\n",
    "    if scatter_sparsity > 0:\n",
    "        ax_main.scatter(X_in[::scatter_sparsity], y_in[::scatter_sparsity], marker='.', c='silver', s=1, alpha=0.5, zorder=10)\n",
    "\n",
    "    # set titles and labels on loss plots\n",
    "    ax_loss.set_title(\"Total Loss\", size=11, fontweight='bold', y=0.9)\n",
    "    ax_loss.set_xlabel(\"Batch\", size=9, fontweight='bold')\n",
    "    ax_loss.set_ylabel(\"Loss\", size=9, fontweight='bold')\n",
    "\n",
    "    ax_params.set_title(\"Batch Loss\", size=11, fontweight='bold', y=0.9)\n",
    "    ax_params.set_xlabel(\"Batch\", size=9, fontweight='bold')\n",
    "    ax_params.set_ylabel(\"Loss\", size=9, fontweight='bold')\n",
    "\n",
    "    # set scale of loss plots\n",
    "    # x axes are logarithimic because progress slows over course of training\n",
    "    ax_loss.set_xscale('log', nonposx='clip')\n",
    "    ax_loss.set_xlim(1, max_batches)\n",
    "    ax_loss.set_ylim(0, loss_scale)        \n",
    "\n",
    "    ax_params.set_xscale('log', nonposx='clip')\n",
    "    ax_params.set_xlim(1, max_batches)\n",
    "    ax_params.set_ylim(0, loss_scale)        \n",
    "\n",
    "    if display_mode == 'file':\n",
    "        plt.savefig(\"%s-%05d.png\" %(filepath, 0))\n",
    "    \n",
    "    # declare components that will be retained between calls\n",
    "    first_pass = True\n",
    "    y_pred_line = None\n",
    "    err_line_u = None\n",
    "    err_line_d = None\n",
    "    fill_between = None\n",
    "    \n",
    "    # RETURN A CALLBACK FUNCTION usable by keras with closure around fixed arguments\n",
    "    def redraw(batch, logs):\n",
    "        \n",
    "        # let Python know that outside scope variables will be used\n",
    "        nonlocal first_pass, y_pred_line, err_line_u, err_line_d, fill_between, tot_batches\n",
    "\n",
    "        # keep track of total number of batches seens\n",
    "        tot_batches += 1\n",
    "                \n",
    "        # update graph at the requested frequency\n",
    "        \n",
    "        if frequency_mode == 'scalar':\n",
    "            if tot_batches % frequency != 0:\n",
    "                return\n",
    "        elif frequency_mode == 'array':\n",
    "            if not frequency[tot_batches]:\n",
    "                return    \n",
    "        \n",
    "        if frequency_mode == 'function':\n",
    "            if not frequency(model, X, y, tot_batches):\n",
    "                return\n",
    "        \n",
    "        # run the model in its current state of training to get the prediction so far\n",
    "        y_pred = model.predict(X).reshape(-1)\n",
    "        \n",
    "    \n",
    "        if show_err:\n",
    "            \n",
    "            # compute the error relative to each training label\n",
    "            err = np.square(y - y_pred.reshape(-1))\n",
    "\n",
    "            # smooth error with a moving average \n",
    "            if err_smoothing > 1:\n",
    "                err = np.convolve(err, np.ones((err_smoothing,))/err_smoothing, mode='same')\n",
    "\n",
    "        # first time through, draw the dynamic portions\n",
    "        if first_pass:\n",
    "\n",
    "            # draw the current prediction of the model\n",
    "            y_pred_line = ax_main.plot(X, y_pred, '-', color='steelblue', lw=4, label='model', zorder=15)[0]\n",
    "\n",
    "            if show_err:\n",
    "\n",
    "                # draw the error around the prediction\n",
    "                err_line_u = ax_main.plot(X, y_pred + err, '-', alpha=0.6, lw=0.5, color='steelblue', label='err', zorder=3)[0]\n",
    "                err_line_d = ax_main.plot(X, y_pred - err, '-', alpha=0.6, lw=0.5, color='steelblue', zorder=3)[0]\n",
    "\n",
    "            if display_mode == 'screen':\n",
    "                plt.show()\n",
    "\n",
    "            first_pass = False\n",
    "\n",
    "        # on subsequent calls, update the dynamic portions\n",
    "        else:\n",
    "\n",
    "            # draw the current prediction of the model\n",
    "            y_pred_line.set_ydata(y_pred)\n",
    "\n",
    "            # update the error around the prediction\n",
    "            if show_err:\n",
    "                err_line_u.set_ydata(y_pred + err)\n",
    "                err_line_d.set_ydata(y_pred - err)\n",
    "\n",
    "        if show_err:\n",
    "            \n",
    "            # shade in the area between the error lines\n",
    "            if fill_between:\n",
    "                fill_between.remove()\n",
    "\n",
    "            fill_between = ax_main.fill_between(X.reshape(-1), y_pred + err, y_pred - err, color='steelblue', alpha=0.2, zorder=0)\n",
    "\n",
    "        # add points to loss graphs\n",
    "        tot_loss = err.sum() / len(y)\n",
    "        ax_loss.scatter([tot_batches], [tot_loss], s=5, c='steelblue')            \n",
    "        ax_params.scatter([tot_batches], [logs['loss']], s=5, c='steelblue')            \n",
    "            \n",
    "        if show_legend:\n",
    "            ax_main.legend()        \n",
    "\n",
    "        if display_mode == 'screen':\n",
    "\n",
    "            # push changes to screen\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "\n",
    "        elif display_mode == 'file':\n",
    "\n",
    "            # save changes to image file        \n",
    "            plt.savefig(\"%s-%05d.png\" % (filepath, tot_batches))\n",
    "    \n",
    "    # return the closure around the callback that Keras will use\n",
    "    return redraw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# initializing to a negative slope so that the training is more interesting\n",
    "initializer = Constant(-1.0)\n",
    "\n",
    "# linear model is an ANN with 1 input node, 1 output node, and linear activation\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=X_train.shape[1], kernel_initializer=initializer, \n",
    "                activation='linear', name='output'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the callback function that will be passed to Keras and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "   256/255480 [..............................] - ETA: 35:36 - loss: 0.7103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (2.071807). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (1.037691). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   512/255480 [..............................] - ETA: 25:05 - loss: 0.6706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.875211). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.439393). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   768/255480 [..............................] - ETA: 21:34 - loss: 0.6463"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.873034). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.438305). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1536/255480 [..............................] - ETA: 18:10 - loss: 0.5801"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.433141). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.433124). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2304/255480 [..............................] - ETA: 16:56 - loss: 0.5219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.432956). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  2560/255480 [..............................] - ETA: 16:46 - loss: 0.5052"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.433729). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  2816/255480 [..............................] - ETA: 16:34 - loss: 0.4916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.438275). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  3072/255480 [..............................] - ETA: 16:22 - loss: 0.4780"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.437352). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255480/255480 [==============================] - 42s 165us/step - loss: 0.0636\n",
      "Epoch 2/25\n",
      "255480/255480 [==============================] - 12s 45us/step - loss: 0.0500\n",
      "Epoch 3/25\n",
      "255480/255480 [==============================] - 10s 38us/step - loss: 0.0460\n",
      "Epoch 4/25\n",
      "255480/255480 [==============================] - 10s 38us/step - loss: 0.0432\n",
      "Epoch 5/25\n",
      "255480/255480 [==============================] - 7s 28us/step - loss: 0.0413\n",
      "Epoch 6/25\n",
      "255480/255480 [==============================] - 7s 27us/step - loss: 0.0399\n",
      "Epoch 7/25\n",
      "255480/255480 [==============================] - 8s 31us/step - loss: 0.0389\n",
      "Epoch 8/25\n",
      "255480/255480 [==============================] - 8s 32us/step - loss: 0.0382\n",
      "Epoch 9/25\n",
      "255480/255480 [==============================] - 7s 29us/step - loss: 0.0378\n",
      "Epoch 10/25\n",
      "255480/255480 [==============================] - 7s 26us/step - loss: 0.0374\n",
      "Epoch 11/25\n",
      "255480/255480 [==============================] - 7s 28us/step - loss: 0.0372\n",
      "Epoch 12/25\n",
      "255480/255480 [==============================] - 7s 28us/step - loss: 0.0371\n",
      "Epoch 13/25\n",
      "255480/255480 [==============================] - 7s 26us/step - loss: 0.0369\n",
      "Epoch 14/25\n",
      "255480/255480 [==============================] - 6s 23us/step - loss: 0.0369\n",
      "Epoch 15/25\n",
      "255480/255480 [==============================] - 7s 28us/step - loss: 0.0368\n",
      "Epoch 16/25\n",
      "255480/255480 [==============================] - 6s 24us/step - loss: 0.0368\n",
      "Epoch 17/25\n",
      "255480/255480 [==============================] - 6s 25us/step - loss: 0.0367\n",
      "Epoch 18/25\n",
      "255480/255480 [==============================] - 7s 28us/step - loss: 0.0367\n",
      "Epoch 19/25\n",
      "255480/255480 [==============================] - 6s 25us/step - loss: 0.0367\n",
      "Epoch 20/25\n",
      "255480/255480 [==============================] - 6s 25us/step - loss: 0.0367\n",
      "Epoch 21/25\n",
      "255480/255480 [==============================] - 6s 25us/step - loss: 0.0367\n",
      "Epoch 22/25\n",
      "255480/255480 [==============================] - 6s 24us/step - loss: 0.0367\n",
      "Epoch 23/25\n",
      "255480/255480 [==============================] - 6s 24us/step - loss: 0.0367\n",
      "Epoch 24/25\n",
      "255480/255480 [==============================] - 6s 24us/step - loss: 0.0367\n",
      "Epoch 25/25\n",
      "255480/255480 [==============================] - 6s 24us/step - loss: 0.0367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2c207240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get closured redraw callback function\n",
    "# this will also draw the background for the graph\n",
    "cb_redraw = get_redraw( X_train, y_train, model, batch_size, epochs,\n",
    "                        frequency=get_frequency_callback(weight_threshold=0.03, bias_threshold=0.02),\n",
    "                        scatter_sparsity=3, show_err=True, err_smoothing=51,\n",
    "                        title=\"Linear Regression of Losses vs. Interest Rate\",\n",
    "                        x_label=\"Interest Rate\",\n",
    "                        y_label=\"Total Loss (% of Funded Amount)\",\n",
    "                        x_tick_formatter=mpl.ticker.PercentFormatter(xmax=1),\n",
    "                        y_tick_formatter=mpl.ticker.PercentFormatter(xmax=1),\n",
    "                        loss_scale=0.8, display_mode='screen')\n",
    "\n",
    "# wrap callback function in Keras structure, to be called after each batch\n",
    "redraw_callback = LambdaCallback(on_batch_end=cb_redraw)\n",
    "\n",
    "# train the model, passing the Keras-wrapped callback function\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[redraw_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
