{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./TrainingANN.png \"Image produced by callback\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Training Visualization\n",
    "\n",
    "This notebook uses a Keras callback and Matplotlib to display an animated graph of a model being trained.<br />\n",
    "\n",
    "The model trained is a multi-layer neural network. The callback function in the cell titled \"Function that draws and updates the graph\" is generic and can be used for any neural network. The key limitations are that it's only for a single input and single output.<br />\n",
    "\n",
    "The basic strategy is to create a hook after each mini-batch training. The callback runs the model on the dataset and plots the results and (if desired) the current mean squared error.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Requirements\n",
    "Note that this uses qt5 for display, not inline. It is possible to do animation inline, but it's a bit more limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib qt5\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data\n",
    "\n",
    "This is a toy dataset made to show how a neural network can fit complex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evenly spaced input values\n",
    "x = np.linspace(0, 12, 10000).reshape(-1,1)\n",
    "\n",
    "# function that maps features to labels\n",
    "def f(x):\n",
    "    # zero, then sin for a bit, then zero again\n",
    "    if x < np.pi:\n",
    "        return 0\n",
    "    elif x < 3 * np.pi:\n",
    "        return np.sin(x)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# map the labels onto the features\n",
    "y = np.array([f(v) for v in x]).reshape(-1)\n",
    "\n",
    "# create labels that are normally distributed around the curve\n",
    "noise = np.random.randn(len(y)) * 0.5\n",
    "\n",
    "features = x\n",
    "labels = y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a quick look at the function and dataset created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1182f0a20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.plot(x, y, lw=3)\n",
    "plt.scatter(features, labels, s=1, alpha=0.5, c='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.reshape(-1, 1), labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that draws and updates the graph\n",
    "The callback function should be run *on_batch_end*. It determines whether an update is necessary and redraws the graph, displaying related data, as well.<br />\n",
    "\n",
    "The enclosing function does some set up and creates a closure with the data that will be retained between calls or that needs to be known in advance, because keras passes very little information to the callback.<br />\n",
    "\n",
    "This function should be reusable for graphing any single-input single-output model in Keras. I have used it, e.g., for multi-layer neural networks.<br />\n",
    "\n",
    "Expect this to be *super slow*. Especially for simple models, the cost of running and graphing the model will be significantly higher than the cost of training on a single batch. You will get warnings from Keras about slowness.<br />\n",
    "\n",
    "This is a toy for learning / investigating. So, performance is not a primary concern, but you do want it to be usable. Several things can have a big impact on performance.<br />\n",
    "<ol>\n",
    "<li>The **sparsity** options reduce the number datapoints used in the scatter plot and in running the model on each pass.</li>\n",
    "<li>The **frequency** option determines how often to update the graph. I've used the function option to update only when the change to the model parameters is big enough to justify an update. But, it's specific to the model being trained.</li>\n",
    "<li>Turning off the **error display** slightly reduces the number of computations, but significantly reduces the amount of drawing.</li>\n",
    "</ol>\n",
    "\n",
    "There is also an option to write the updates to files as individual images, which can then be used to create an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# prevents overflow when displaying a lot of data points\n",
    "mpl.rcParams['agg.path.chunksize'] = 100000\n",
    "\n",
    "# do some set up and return a closure around the redraw callback for Keras \n",
    "def get_redraw(X_in, y_in, model, batch_size, epochs, **kwargs):\n",
    "\n",
    "    ## PROCESS COMMAND LINE ARGUMENTS\n",
    "    \n",
    "    # plot dimensions\n",
    "    left = kwargs.get('left', X_in.min())\n",
    "    right = kwargs.get('right', X_in.max())\n",
    "    bottom = kwargs.get('bottom', y_in.min())\n",
    "    top = kwargs.get('top', y_in.max())\n",
    "\n",
    "    # how much data to use in graph\n",
    "    \n",
    "    # ... scatter plot sparsity (0 = no scatter plot)\n",
    "    #     scatter is only drawn once, but it can be a lot of data, both computationally and visually\n",
    "    scatter_sparsity = kwargs.get('scatter_sparsity', 5)\n",
    "\n",
    "    # ... graph sparsity\n",
    "    #     keeping the graph sparse improves performance\n",
    "    graph_sparsity = kwargs.get('graph_sparsity', 1000)\n",
    "\n",
    "    # whether to display error\n",
    "    show_err = kwargs.get('show_err', True)\n",
    "    \n",
    "    # .. and level of smoothing to apply to error, if so (needs to be an odd number)\n",
    "    err_smoothing = kwargs.get('err_smoothing', 101)\n",
    "    \n",
    "    # how frequently (in batches) to update the graph\n",
    "    frequency = kwargs.get('frequency', 10)\n",
    "    if callable(frequency):\n",
    "        # if a function is provided, it will be called every batch and asked for a True/False response\n",
    "        frequency_mode = 'function'\n",
    "    elif np.isscalar(frequency):\n",
    "        # if a number is provided, updates will be done every [frequency] batches\n",
    "        frequency_mode = 'scalar'\n",
    "    else:\n",
    "        # for array-like setting, update when frequency[batch number] is True\n",
    "        frequency_mode = 'array'\n",
    "        \n",
    "    # figure size\n",
    "    figure_size = kwargs.get('figure_size', (15, 10))\n",
    "    \n",
    "    # text labels\n",
    "    title = kwargs.get('title', None)\n",
    "    x_label = kwargs.get('x_label', None)\n",
    "    y_label = kwargs.get('y_label', None)\n",
    "    \n",
    "    # tick formatters\n",
    "    x_tick_formatter = kwargs.get('x_tick_formatter', None)\n",
    "    y_tick_formatter = kwargs.get('y_tick_formatter', None)\n",
    "    \n",
    "    # loss scale (depends on loss function)\n",
    "    loss_scale = kwargs.get('loss_scale', 1.0)\n",
    "    \n",
    "    # display legend\n",
    "    show_legend = kwargs.get('show_legend', True)\n",
    "    \n",
    "    # write to screen or file?\n",
    "    display_mode = kwargs.get('display_mode', 'screen')\n",
    "    filepath = kwargs.get('filepath', 'images/batch')\n",
    "    \n",
    "    \n",
    "    ## PREP DATA FOR QUICKER DISPLAY\n",
    "    \n",
    "    # parallel sort feature and label arrays for plotting\n",
    "    ix = X_in.argsort(axis=0)[:,0]\n",
    "\n",
    "    # ... reducing number of points used in graph\n",
    "    ix = ix[::graph_sparsity]\n",
    "    \n",
    "    X = X_in[ix]\n",
    "    y = y_in[ix]\n",
    "    \n",
    "    # keep track of total number of batches seens\n",
    "    tot_batches = 0\n",
    "    batches_per_epoch = np.ceil(len(X_in) / batch_size)\n",
    "    \n",
    "    # scale for loss plot = total number of batches that will be run\n",
    "    max_batches = epochs * batches_per_epoch\n",
    "    \n",
    "    ## DRAW BACKGROUND COMPONENTS\n",
    "    \n",
    "    # set the figure size and layout\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    grd = gridspec.GridSpec(ncols=3, nrows=2)\n",
    "    \n",
    "    # graphs for data/error, loss over time, and model parameters\n",
    "    ax_main = fig.add_subplot(grd[:2, :2])\n",
    "    ax_loss = fig.add_subplot(grd[:1, 2:])\n",
    "    ax_params = fig.add_subplot(grd[1:, 2:])\n",
    "\n",
    "    # data boundaries on main graph\n",
    "    ax_main.set_xlim(left, right)\n",
    "    ax_main.set_ylim(bottom, top)\n",
    "    \n",
    "    # titles and labels on main graph\n",
    "    if title:\n",
    "        ax_main.set_title(title, size=14, fontweight='bold', y=1.03)\n",
    "    \n",
    "    if x_label:\n",
    "        ax_main.set_xlabel(x_label, size=12, fontweight='bold')\n",
    "        \n",
    "    if y_label:\n",
    "        ax_main.set_ylabel(y_label, size=12, fontweight='bold')\n",
    "                 \n",
    "    # tick formatting on main graph\n",
    "    if x_tick_formatter:\n",
    "        ax_main.xaxis.set_major_formatter(x_tick_formatter)\n",
    "        \n",
    "    if y_tick_formatter:\n",
    "        ax_main.yaxis.set_major_formatter(y_tick_formatter)\n",
    "        \n",
    "    # draw a scatter plot of the training data on main graph\n",
    "    if scatter_sparsity > 0:\n",
    "        ax_main.scatter(X_in[::scatter_sparsity], y_in[::scatter_sparsity], marker='.', c='silver', s=1, alpha=0.5, zorder=10)\n",
    "\n",
    "    # set titles and labels on loss plots\n",
    "    ax_loss.set_title(\"Total Loss\", size=11, fontweight='bold', y=0.9)\n",
    "    ax_loss.set_xlabel(\"Batch\", size=9, fontweight='bold')\n",
    "    ax_loss.set_ylabel(\"Loss\", size=9, fontweight='bold')\n",
    "\n",
    "    ax_params.set_title(\"Batch Loss\", size=11, fontweight='bold', y=0.9)\n",
    "    ax_params.set_xlabel(\"Batch\", size=9, fontweight='bold')\n",
    "    ax_params.set_ylabel(\"Loss\", size=9, fontweight='bold')\n",
    "\n",
    "    # set scale of loss plots\n",
    "    # x axes are logarithimic because progress slows over course of training\n",
    "    ax_loss.set_xscale('log', nonposx='clip')\n",
    "    ax_loss.set_xlim(1, max_batches)\n",
    "    ax_loss.set_ylim(0, loss_scale)        \n",
    "\n",
    "    ax_params.set_xscale('log', nonposx='clip')\n",
    "    ax_params.set_xlim(1, max_batches)\n",
    "    ax_params.set_ylim(0, loss_scale)        \n",
    "\n",
    "    if display_mode == 'file':\n",
    "        plt.savefig(\"%s-%05d.png\" %(filepath, 0))\n",
    "    \n",
    "    # declare components that will be retained between calls\n",
    "    first_pass = True\n",
    "    y_pred_line = None\n",
    "    err_line_u = None\n",
    "    err_line_d = None\n",
    "    fill_between = None\n",
    "    \n",
    "    # RETURN A CALLBACK FUNCTION usable by keras with closure around fixed arguments\n",
    "    def redraw(batch, logs):\n",
    "        \n",
    "        # let Python know that outside scope variables will be used\n",
    "        nonlocal first_pass, y_pred_line, err_line_u, err_line_d, fill_between, tot_batches\n",
    "\n",
    "        # keep track of total number of batches seens\n",
    "        tot_batches += 1\n",
    "                \n",
    "        # update graph at the requested frequency\n",
    "        \n",
    "        if frequency_mode == 'scalar':\n",
    "            if tot_batches % frequency != 0:\n",
    "                return\n",
    "        elif frequency_mode == 'array':\n",
    "            if not frequency[tot_batches]:\n",
    "                return    \n",
    "        \n",
    "        if frequency_mode == 'function':\n",
    "            if not frequency(model, X, y, tot_batches):\n",
    "                return\n",
    "        \n",
    "        # run the model in its current state of training to get the prediction so far\n",
    "        y_pred = model.predict(X).reshape(-1)\n",
    "        \n",
    "    \n",
    "        if show_err:\n",
    "            \n",
    "            # compute the error relative to each training label\n",
    "            err = np.square(y - y_pred.reshape(-1))\n",
    "\n",
    "            # smooth error with a moving average \n",
    "            if err_smoothing > 1:\n",
    "                err = np.convolve(err, np.ones((err_smoothing,))/err_smoothing, mode='same')\n",
    "\n",
    "        # first time through, draw the dynamic portions\n",
    "        if first_pass:\n",
    "\n",
    "            # draw the current prediction of the model\n",
    "            y_pred_line = ax_main.plot(X, y_pred, '-', color='steelblue', lw=4, label='model', zorder=15)[0]\n",
    "\n",
    "            if show_err:\n",
    "\n",
    "                # draw the error around the prediction\n",
    "                err_line_u = ax_main.plot(X, y_pred + err, '-', alpha=0.6, lw=0.5, color='steelblue', label='err', zorder=3)[0]\n",
    "                err_line_d = ax_main.plot(X, y_pred - err, '-', alpha=0.6, lw=0.5, color='steelblue', zorder=3)[0]\n",
    "\n",
    "            if display_mode == 'screen':\n",
    "                plt.show()\n",
    "\n",
    "            first_pass = False\n",
    "\n",
    "        # on subsequent calls, update the dynamic portions\n",
    "        else:\n",
    "\n",
    "            # draw the current prediction of the model\n",
    "            y_pred_line.set_ydata(y_pred)\n",
    "\n",
    "            # update the error around the prediction\n",
    "            if show_err:\n",
    "                err_line_u.set_ydata(y_pred + err)\n",
    "                err_line_d.set_ydata(y_pred - err)\n",
    "\n",
    "        if show_err:\n",
    "            \n",
    "            # shade in the area between the error lines\n",
    "            if fill_between:\n",
    "                fill_between.remove()\n",
    "\n",
    "            fill_between = ax_main.fill_between(X.reshape(-1), y_pred + err, y_pred - err, color='steelblue', alpha=0.2, zorder=0)\n",
    "\n",
    "        # add points to loss graphs\n",
    "        tot_loss = err.sum() / len(y)\n",
    "        ax_loss.scatter([tot_batches], [tot_loss], s=5, c='steelblue')            \n",
    "        ax_params.scatter([tot_batches], [logs['loss']], s=5, c='steelblue')            \n",
    "            \n",
    "        if show_legend:\n",
    "            ax_main.legend()        \n",
    "\n",
    "        if display_mode == 'screen':\n",
    "\n",
    "            # push changes to screen\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "\n",
    "        elif display_mode == 'file':\n",
    "\n",
    "            # save changes to image file        \n",
    "            plt.savefig(\"%s-%05d.png\" % (filepath, tot_batches))\n",
    "    \n",
    "    # return the closure around the callback that Keras will use\n",
    "    return redraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=features.shape[1], activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adagrad')\n",
    "\n",
    "epochs = 75\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the callback function that will be passed to Keras and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "7500/7500 [==============================] - 1s 186us/step - loss: 1.1196\n",
      "Epoch 2/75\n",
      " 256/7500 [>.............................] - ETA: 8s - loss: 0.5236"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.283951). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.143327). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.5011\n",
      "Epoch 3/75\n",
      " 384/7500 [>.............................] - ETA: 5s - loss: 0.4908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.144664). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.4980\n",
      "Epoch 4/75\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.4917\n",
      "Epoch 5/75\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 0.4874\n",
      "Epoch 6/75\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 0.4776\n",
      "Epoch 7/75\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.4741\n",
      "Epoch 8/75\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 0.4677\n",
      "Epoch 9/75\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.4670\n",
      "Epoch 10/75\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.4587\n",
      "Epoch 11/75\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.4582\n",
      "Epoch 12/75\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 0.4498\n",
      "Epoch 13/75\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.4499\n",
      "Epoch 14/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.4415\n",
      "Epoch 15/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.4354\n",
      "Epoch 16/75\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 0.4290\n",
      "Epoch 17/75\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.4218\n",
      "Epoch 18/75\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.4146\n",
      "Epoch 19/75\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.4061\n",
      "Epoch 20/75\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.3968\n",
      "Epoch 21/75\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.3892\n",
      "Epoch 22/75\n",
      " 256/7500 [>.............................] - ETA: 8s - loss: 0.3786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.311183). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.156782). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.3824\n",
      "Epoch 23/75\n",
      " 384/7500 [>.............................] - ETA: 5s - loss: 0.3960"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.152295). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.3731\n",
      "Epoch 24/75\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 0.3660\n",
      "Epoch 25/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.3590\n",
      "Epoch 26/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.3513\n",
      "Epoch 27/75\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.3432\n",
      "Epoch 28/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.3346\n",
      "Epoch 29/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.3273\n",
      "Epoch 30/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.3190\n",
      "Epoch 31/75\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.3103\n",
      "Epoch 32/75\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 0.3120\n",
      "Epoch 33/75\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.2985\n",
      "Epoch 34/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.3021\n",
      "Epoch 35/75\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.2904\n",
      "Epoch 36/75\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.2913\n",
      "Epoch 37/75\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.2841\n",
      "Epoch 38/75\n",
      "7500/7500 [==============================] - 1s 155us/step - loss: 0.2830\n",
      "Epoch 39/75\n",
      "7500/7500 [==============================] - 31s 4ms/step - loss: 0.2801\n",
      "Epoch 40/75\n",
      "7500/7500 [==============================] - 1s 173us/step - loss: 0.2773\n",
      "Epoch 41/75\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 0.2767\n",
      "Epoch 42/75\n",
      " 256/7500 [>.............................] - ETA: 9s - loss: 0.2740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.340571). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.171572). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 176us/step - loss: 0.2755\n",
      "Epoch 43/75\n",
      " 384/7500 [>.............................] - ETA: 6s - loss: 0.2871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.174506). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 155us/step - loss: 0.2825\n",
      "Epoch 44/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.2803\n",
      "Epoch 45/75\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.2735\n",
      "Epoch 46/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 0.2774\n",
      "Epoch 47/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.2740\n",
      "Epoch 48/75\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.2735\n",
      "Epoch 49/75\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 0.2737\n",
      "Epoch 50/75\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.2735\n",
      "Epoch 51/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 0.2747\n",
      "Epoch 52/75\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.2720\n",
      "Epoch 53/75\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 0.2718\n",
      "Epoch 54/75\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 0.2731\n",
      "Epoch 55/75\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 0.2735\n",
      "Epoch 56/75\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 0.2715\n",
      "Epoch 57/75\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 0.2725\n",
      "Epoch 58/75\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 0.2704\n",
      "Epoch 59/75\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 0.2701\n",
      "Epoch 60/75\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 0.2709\n",
      "Epoch 61/75\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 0.2712\n",
      "Epoch 62/75\n",
      " 256/7500 [>.............................] - ETA: 10s - loss: 0.2664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.357364). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.179842). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 164us/step - loss: 0.2709\n",
      "Epoch 63/75\n",
      " 384/7500 [>.............................] - ETA: 6s - loss: 0.2660"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.179960). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 161us/step - loss: 0.2720\n",
      "Epoch 64/75\n",
      "7500/7500 [==============================] - 1s 169us/step - loss: 0.2702\n",
      "Epoch 65/75\n",
      "7500/7500 [==============================] - 1s 164us/step - loss: 0.2712\n",
      "Epoch 66/75\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 0.2705\n",
      "Epoch 67/75\n",
      "7500/7500 [==============================] - 1s 170us/step - loss: 0.2698\n",
      "Epoch 68/75\n",
      "7500/7500 [==============================] - 1s 173us/step - loss: 0.2718\n",
      "Epoch 69/75\n",
      "7500/7500 [==============================] - 1s 171us/step - loss: 0.2700\n",
      "Epoch 70/75\n",
      "7500/7500 [==============================] - 1s 168us/step - loss: 0.2706\n",
      "Epoch 71/75\n",
      "7500/7500 [==============================] - 1s 168us/step - loss: 0.2701\n",
      "Epoch 72/75\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 0.2687\n",
      "Epoch 73/75\n",
      "7500/7500 [==============================] - 1s 170us/step - loss: 0.2701\n",
      "Epoch 74/75\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 0.2685\n",
      "Epoch 75/75\n",
      "7500/7500 [==============================] - 1s 168us/step - loss: 0.2695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a354d2240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "# get closured redraw callback function\n",
    "# this will also draw the background for the graph\n",
    "cb_redraw = get_redraw( X_train, y_train, model, batch_size, epochs,\n",
    "                        frequency=20, graph_sparsity=1,\n",
    "                        scatter_sparsity=1, show_err=True, err_smoothing=201,\n",
    "                        title=\"Neural Network Fitting Complex Function\",\n",
    "                        x_label=\"x\",\n",
    "                        y_label=\"f(x)\",\n",
    "                        loss_scale=0.8, display_mode='screen')\n",
    "\n",
    "# wrap callback function in Keras structure, to be called after each batch\n",
    "redraw_callback = LambdaCallback(on_batch_end=cb_redraw)\n",
    "\n",
    "# train the model, passing the Keras-wrapped callback function\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[redraw_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
